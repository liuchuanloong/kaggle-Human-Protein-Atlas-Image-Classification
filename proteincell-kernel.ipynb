{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fca22a7cd11c943247068ff3c5117e4587bf3d1"
   },
   "source": [
    "#### Image size was made smaller for notebook use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95cb8bc95d37dd88806ca1f1c61817a2a93e4f39",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pathlib\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "# from neptune import Context\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim, save\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from ignite.engine import Events\n",
    "from ignite.engine import create_supervised_evaluator, create_supervised_trainer\n",
    "from ignite.metrics import CategoricalAccuracy, Recall, Precision\n",
    "from ignite.metrics import Loss\n",
    "import numpy as np\n",
    "RANDOM_SEED = 666\n",
    "\n",
    "LABEL_MAP = {\n",
    "0: \"Nucleoplasm\" ,\n",
    "1: \"Nuclear membrane\"   ,\n",
    "2: \"Nucleoli\"   ,\n",
    "3: \"Nucleoli fibrillar center\",   \n",
    "4: \"Nuclear speckles\"   ,\n",
    "5: \"Nuclear bodies\"   ,\n",
    "6: \"Endoplasmic reticulum\"   ,\n",
    "7: \"Golgi apparatus\"  ,\n",
    "8: \"Peroxisomes\"   ,\n",
    "9:  \"Endosomes\"   ,\n",
    "10: \"Lysosomes\"   ,\n",
    "11: \"Intermediate filaments\"  , \n",
    "12: \"Actin filaments\"   ,\n",
    "13: \"Focal adhesion sites\"  ,\n",
    "14: \"Microtubules\"   ,\n",
    "15: \"Microtubule ends\"   ,\n",
    "16: \"Cytokinetic bridge\"   ,\n",
    "17: \"Mitotic spindle\"  ,\n",
    "18: \"Microtubule organizing center\",  \n",
    "19: \"Centrosome\",\n",
    "20: \"Lipid droplets\"   ,\n",
    "21: \"Plasma membrane\"  ,\n",
    "22: \"Cell junctions\"   ,\n",
    "23: \"Mitochondria\"   ,\n",
    "24: \"Aggresome\"   ,\n",
    "25: \"Cytosol\" ,\n",
    "26: \"Cytoplasmic bodies\",\n",
    "27: \"Rods & rings\"}\n",
    "\n",
    "class MultiBandMultiLabelDataset(Dataset):\n",
    "    BANDS_NAMES = ['_red.png','_green.png','_blue.png','_yellow.png']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_df)\n",
    "    \n",
    "    def __init__(self, images_df, \n",
    "                 base_path, \n",
    "                 image_transform, \n",
    "                 augmentator=None,\n",
    "                 train_mode=True    \n",
    "                ):\n",
    "        if not isinstance(base_path, pathlib.Path):\n",
    "            base_path = pathlib.Path(base_path)\n",
    "            \n",
    "        self.images_df = images_df.copy()\n",
    "        self.image_transform = image_transform\n",
    "        self.augmentator = augmentator\n",
    "        self.images_df.Id = self.images_df.Id.apply(lambda x: base_path / x)\n",
    "        self.mlb = MultiLabelBinarizer(classes=list(LABEL_MAP.keys()))\n",
    "        self.train_mode = train_mode                                      \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        y = None\n",
    "        X = self._load_multiband_image(index)\n",
    "        if self.train_mode:\n",
    "            y = self._load_multilabel_target(index)\n",
    "        \n",
    "        # augmentator can be for instance imgaug augmentation object\n",
    "        if self.augmentator is not None:\n",
    "            X = self.augmentator(X)\n",
    "            \n",
    "        X = self.image_transform(X)\n",
    "            \n",
    "        return X, y \n",
    "        \n",
    "    def _load_multiband_image(self, index):\n",
    "        row = self.images_df.iloc[index]\n",
    "        image_bands = []\n",
    "        for band_name in self.BANDS_NAMES:\n",
    "            p = str(row.Id.absolute()) + band_name\n",
    "            pil_channel = PIL.Image.open(p)\n",
    "            image_bands.append(pil_channel)\n",
    "            \n",
    "        # lets pretend its a RBGA image to support 4 channels\n",
    "        band4image = PIL.Image.merge('RGBA', bands=image_bands)\n",
    "        return band4image\n",
    "    \n",
    "    \n",
    "    def _load_multilabel_target(self, index):\n",
    "        return list(map(int, self.images_df.iloc[index].Target.split(' ')))\n",
    "    \n",
    "        \n",
    "    def collate_func(self, batch):\n",
    "        labels = None\n",
    "        images = [x[0] for x in batch]\n",
    "        \n",
    "        if self.train_mode:\n",
    "            labels = [x[1] for x in batch]\n",
    "            labels_one_hot  = self.mlb.fit_transform(labels)\n",
    "            labels = torch.FloatTensor(labels_one_hot)\n",
    "            \n",
    "        \n",
    "        return torch.stack(images)[:,:4,:,:], labels\n",
    "\n",
    "def get_model(n_classes, image_channels=4):\n",
    "    model = resnet50(pretrained=False)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    inft = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features=inft, out_features=n_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "    \n",
    "    return model  \n",
    "\n",
    "\n",
    "def train(trainer, train_loader, test_loader, checkpoint_path='bestmodel_{}_{}.torch', epochs=1):\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_loss(engine):\n",
    "        iter = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "#         ctx.channel_send('loss', engine.state.output)\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "                  \"\".format(engine.state.epoch, iter, len(train_loader), engine.state.output))\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(test_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_nll = metrics['loss']\n",
    "        print(\"Training Results - Epoch: {}  Avg loss: {:.2f}\"\n",
    "              .format(engine.state.epoch, avg_nll))\n",
    "        save(model, checkpoint_path.format(engine.state.epoch, avg_nll))\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "    \n",
    "    return model \n",
    "    \n",
    "\n",
    "# Eval\n",
    "def evaluate(model, test_loader, threshold=0.2):\n",
    "    all_preds = []\n",
    "    true = []\n",
    "    model.eval()\n",
    "    for b in test_loader:\n",
    "        X, y = b\n",
    "        if torch.cuda.is_available():\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        pred = model(X)\n",
    "        all_preds.append(pred.sigmoid().cpu().data.numpy())\n",
    "        true.append(y.cpu().data.numpy())\n",
    "        \n",
    "        \n",
    "    P = np.concatenate(all_preds)\n",
    "    R = np.concatenate(true)\n",
    "    \n",
    "    f1 = f1_score(P>threshold, R, average='macro')\n",
    "    print(f1)\n",
    "    return f1\n",
    "    \n",
    "\n",
    "## Submission\n",
    "def predict_submission(model, submission_load):\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    for i, b in enumerate(submission_load):\n",
    "        if i % 100: print('processing batch {}/{}'.format(i, len(submission_load)))\n",
    "        X, _ = b\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "        pred = model(X)\n",
    "        all_preds.append(pred.sigmoid().cpu().data.numpy())\n",
    "    return np.concatenate(all_preds)\n",
    "        \n",
    "         \n",
    "def make_submission_file(sample_submission_df, predictions):\n",
    "    submissions = []\n",
    "    for row in predictions:\n",
    "        subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
    "        submissions.append(subrow)\n",
    "    \n",
    "    sample_submission_df['Predicted'] = submissions\n",
    "    sample_submission_df.to_csv('submission.csv', index=None)\n",
    "    \n",
    "    return sample_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf8572e407560153edaef6727735f5a69f606a4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataframe files\n",
    "\n",
    "PATH_TO_IMAGES = '../input/train/'\n",
    "PATH_TO_TEST_IMAGES = '../input/test/'\n",
    "PATH_TO_META = '../input/train.csv'\n",
    "SAMPLE_SUBMI = '../input/sample_submission.csv'\n",
    "\n",
    "SEED = 666\n",
    "DEV_MODE = True\n",
    "    \n",
    "df = pd.read_csv(PATH_TO_META)\n",
    "df_train, df_test  = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "df_submission = pd.read_csv(SAMPLE_SUBMI)\n",
    "\n",
    "if DEV_MODE:\n",
    "    df_train = df_train[:200]\n",
    "    df_test = df_test[:50]\n",
    "    df_submission = df_submission[:50]\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "    \n",
    "        ])\n",
    "\n",
    " \n",
    "# Prepare datasets and loaders\n",
    "   \n",
    "gtrain = MultiBandMultiLabelDataset(df_train, base_path=PATH_TO_IMAGES, image_transform=image_transform)\n",
    "gtest = MultiBandMultiLabelDataset(df_test, base_path=PATH_TO_IMAGES, image_transform=image_transform)\n",
    "gsub = MultiBandMultiLabelDataset(df_submission, base_path=PATH_TO_TEST_IMAGES, train_mode=False, image_transform=image_transform)\n",
    "\n",
    "train_load = DataLoader(gtrain, collate_fn=gtrain.collate_func, batch_size=16, num_workers=6)\n",
    "test_load = DataLoader(gtest, collate_fn=gtest.collate_func, batch_size=16, num_workers=6)\n",
    "submission_load = DataLoader(gsub, collate_fn=gsub.collate_func, batch_size=16, num_workers=6)\n",
    "\n",
    "\n",
    "# Prepare model \n",
    "\n",
    "model = get_model(28,4)\n",
    "device='cpu'\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "if torch.cuda.is_available():\n",
    "    criterion = criterion.cuda()\n",
    "evaluator = create_supervised_evaluator(model,\n",
    "                                            device=device,\n",
    "                                            metrics={'loss': Loss(criterion)\n",
    "                                                    })\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=0.00005)\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e530a84dce9fa6374195fab79d8084ddaef9c28d"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd3c3756df0888108cc47474679b4a4ac29fb922",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = train(trainer, train_load, test_load, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8647728f845816ae2faf0dfccc8c74d9babec3d4"
   },
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62041cbf27e649f9cb15500cebc92579f0d0227c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on testing data and calculate F1-macro\n",
    "res = evaluate(model, test_load, threshold=0.2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00dd917d95ab8d3e1d45fe8dd2a3010607de883f"
   },
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96ff86d6ba72a4f6f2e0d59c7aa9041b7b100827",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_predictions =predict_submission(model, submission_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47e9c81bd9ee2610d174ddb89fe44dfa786c4d28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the submission file and \n",
    "THRESHOLD = 0.2\n",
    "p = submission_predictions>THRESHOLD\n",
    "\n",
    "submission_file = make_submission_file(sample_submission_df=df_submission,\n",
    "                     predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd831c044230459f54559a11e9424dc54324f932",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_file.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
